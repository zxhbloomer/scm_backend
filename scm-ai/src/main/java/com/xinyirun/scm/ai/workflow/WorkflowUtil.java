package com.xinyirun.scm.ai.workflow;

import cn.hutool.extra.spring.SpringUtil;
import com.xinyirun.scm.ai.bean.vo.workflow.AiWorkflowNodeVo;
import com.xinyirun.scm.ai.bean.vo.request.AIChatOptionVo;
import com.xinyirun.scm.ai.bean.vo.request.AIChatRequestVo;
import com.xinyirun.scm.ai.config.memory.ScmWorkflowMessageChatMemory;
import com.xinyirun.scm.ai.core.service.chat.AiChatBaseService;
import com.xinyirun.scm.ai.workflow.data.NodeIOData;
import com.xinyirun.scm.ai.workflow.data.NodeIODataContent;
import com.xinyirun.scm.common.utils.datasource.DataSourceHelper;
import lombok.extern.slf4j.Slf4j;
import org.apache.commons.lang3.StringUtils;

import java.util.List;
import java.util.Map;

import static com.xinyirun.scm.ai.workflow.WorkflowConstants.DEFAULT_OUTPUT_PARAM_NAME;

/**
 * å·¥ä½œæµå·¥å…·ç±»
 *
 * æä¾›å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­å¸¸ç”¨çš„åŠŸèƒ½ï¼š
 * 1. æ¨¡æ¿æ¸²æŸ“ - æ”¯æŒ ${paramName} æ ¼å¼çš„å˜é‡æ›¿æ¢
 * 2. LLM è°ƒç”¨ - è°ƒç”¨ AI æ¨¡å‹ç”Ÿæˆå“åº”
 *
 * æ³¨æ„ï¼šæ‰€æœ‰æ–¹æ³•å‡ä¸ºé™æ€æ–¹æ³•ï¼Œä¸éœ€è¦ Spring ç»„ä»¶æ³¨å†Œ
 */
@Slf4j
public class WorkflowUtil {

    /**
     * æ¸²æŸ“æ¨¡æ¿å­—ç¬¦ä¸²
     *
     * å°†æ¨¡æ¿ä¸­çš„ ${paramName} æ›¿æ¢ä¸ºå®é™…çš„å‚æ•°å€¼
     * æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼ˆæ–‡æœ¬ã€æ–‡ä»¶åˆ—è¡¨ã€é€‰é¡¹ç­‰ï¼‰
     *
     * @param template åŒ…å« ${paramName} æ ¼å¼çš„æ¨¡æ¿å­—ç¬¦ä¸²
     * @param values åŒ…å«å‚æ•°æ•°æ®çš„ NodeIOData åˆ—è¡¨
     * @return æ¸²æŸ“åçš„å­—ç¬¦ä¸²
     */
    public static String renderTemplate(String template, List<NodeIOData> values) {
        String result = template;
        for (NodeIOData next : values) {
            String name = next.getName();
            NodeIODataContent<?> dataContent = (NodeIODataContent<?>) next.getContent();

            if (dataContent.getValue() instanceof List) {
                List<?> list = (List<?>) dataContent.getValue();
                String joinedValue = String.join(",", list.stream()
                        .map(Object::toString)
                        .toArray(String[]::new));
                result = result.replace("${" + name + "}", joinedValue);
            } else if (dataContent.getValue() != null) {
                result = result.replace("${" + name + "}", dataContent.getValue().toString());
            }
        }
        return result;
    }

    /**
     * è°ƒç”¨ LLM æ¨¡å‹ç”Ÿæˆå“åº”ï¼ˆéæµå¼ï¼‰
     *
     * é€šè¿‡ AiChatBaseService è°ƒç”¨é…ç½®çš„ LLM æ¨¡å‹ï¼Œè·å– AI ç”Ÿæˆçš„å“åº”å†…å®¹
     * ç”¨äºéœ€è¦è·å–å®Œæ•´å“åº”çš„åœºæ™¯ï¼ˆå¦‚åˆ†ç±»å™¨èŠ‚ç‚¹ï¼‰
     *
     * @param wfState å·¥ä½œæµçŠ¶æ€å¯¹è±¡
     * @param modelName æ¨¡å‹åç§°
     * @param prompt æç¤ºè¯/é—®é¢˜
     * @return åŒ…å« LLM å“åº”çš„ NodeIOData å¯¹è±¡
     */
    public static NodeIOData invokeLLM(WfState wfState, String modelName, String prompt) {
        log.info("invoke LLM (non-streaming), modelName: {}, prompt length: {}", modelName,
                StringUtils.isNotBlank(prompt) ? prompt.length() : 0);

        try {
            AIChatRequestVo request = new AIChatRequestVo();
            request.setAiType("LLM");

            AiChatBaseService aiChatBaseService = SpringUtil.getBean(AiChatBaseService.class);
            if (aiChatBaseService == null) {
                throw new RuntimeException("AiChatBaseService not found in Spring context");
            }

            var modelConfig = aiChatBaseService.getModule(request, null);

            AIChatOptionVo chatOption = new AIChatOptionVo();
            chatOption.setModule(modelConfig);
            chatOption.setPrompt(prompt);

            String response = aiChatBaseService.chat(chatOption).content();

            log.info("LLM response length: {}", StringUtils.isNotBlank(response) ? response.length() : 0);

            return NodeIOData.createByText(DEFAULT_OUTPUT_PARAM_NAME, "", response);
        } catch (Exception e) {
            log.error("invoke LLM failed", e);
            throw new RuntimeException("LLM è°ƒç”¨å¤±è´¥: " + e.getMessage(), e);
        }
    }

    /**
     * æµå¼è°ƒç”¨ LLM æ¨¡å‹ç”Ÿæˆå“åº”
     *
     * <p>æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†åŠŸèƒ½ï¼š</p>
     * <ul>
     *   <li>ä» wfState è·å– conversationId</li>
     *   <li>LLM è°ƒç”¨å‰ä¿å­˜ USER æ¶ˆæ¯</li>
     *   <li>ä½¿ç”¨ chatWithMemoryStream() è°ƒç”¨ LLMï¼ˆå¸¦è®°å¿†ï¼‰</li>
     *   <li>LLM è°ƒç”¨åä¿å­˜ ASSISTANT æ¶ˆæ¯</li>
     *   <li>é™çº§æ¨¡å¼: conversationId ä¸º NULL æ—¶ä½¿ç”¨ chatStream()ï¼ˆæ— è®°å¿†ï¼‰</li>
     * </ul>
     *
     * @param wfState å·¥ä½œæµçŠ¶æ€å¯¹è±¡
     * @param nodeState å·¥ä½œæµèŠ‚ç‚¹çŠ¶æ€
     * @param node å·¥ä½œæµèŠ‚ç‚¹å®šä¹‰
     * @param modelName æ¨¡å‹åç§°
     * @param prompt æç¤ºè¯/é—®é¢˜
     */
    public static void streamingInvokeLLM(WfState wfState, WfNodeState nodeState, AiWorkflowNodeVo node,
                                           String modelName, String prompt) {
        String conversationId = wfState.getConversationId();

        // æå–åŸå§‹ç”¨æˆ·è¾“å…¥ï¼ˆç”¨äºå¯¹è¯è®°å½•ï¼Œè€Œä¸æ˜¯æ¸²æŸ“åçš„promptï¼‰
        String originalUserInput = extractOriginalUserInput(wfState);

        log.info("invoke LLM (streaming), modelName: {}, conversationId: {}, originalUserInput length: {}, prompt length: {}",
                modelName, conversationId,
                originalUserInput != null ? originalUserInput.length() : 0,
                StringUtils.isNotBlank(prompt) ? prompt.length() : 0);

        try {
            AIChatRequestVo request = new AIChatRequestVo();
            request.setAiType("LLM");

            AiChatBaseService aiChatBaseService = SpringUtil.getBean(AiChatBaseService.class);
            if (aiChatBaseService == null) {
                throw new RuntimeException("AiChatBaseService not found in Spring context");
            }

            var modelConfig = aiChatBaseService.getModule(request, null);

            AIChatOptionVo chatOption = new AIChatOptionVo();
            chatOption.setModule(modelConfig);
            chatOption.setPrompt(prompt);

            StringBuilder fullResponse = new StringBuilder();

            // é™çº§æ¨¡å¼åˆ¤æ–­: conversationId ä¸º NULL æ—¶ä½¿ç”¨æ— è®°å¿†æ¨¡å¼
            if (StringUtils.isBlank(conversationId)) {
                log.warn("conversationId is null, fallback to no-memory mode");

                // æ— è®°å¿†æ¨¡å¼ - ä½¿ç”¨ chatStream()
                aiChatBaseService.chatStream(chatOption)
                        .chatResponse()
                        .doOnNext(chatResponse -> {
                            // åœ¨Reactoræµå›è°ƒä¸­è®¾ç½®ç§Ÿæˆ·ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢çº¿ç¨‹åˆ‡æ¢å¯¼è‡´ä¸Šä¸‹æ–‡ä¸¢å¤±
                            if (wfState.getTenantCode() != null) {
                                DataSourceHelper.use(wfState.getTenantCode());
                            }

                            String content = chatResponse.getResult().getOutput().getText();
                            if (StringUtils.isNotBlank(content)) {
                                log.debug("LLM chunk: length={}", content.length());
                                fullResponse.append(content);

                                if (wfState.getStreamHandler() != null) {
                                    wfState.getStreamHandler().sendNodeChunk(node.getUuid(), content);
                                }
                            }
                        })
                        .blockLast();
            } else {
                // æœ‰è®°å¿†æ¨¡å¼ - ä½¿ç”¨ chatWithWorkflowMemoryStream()
                // WorkflowConversationAdvisorä¼šé€šè¿‡å‚æ•°ä¼ é€’runtime_uuidä¿å­˜å¯¹è¯è®°å½•

                // è®¾ç½® conversationId å¹¶è°ƒç”¨ LLMï¼ˆä½¿ç”¨Workflowä¸“ç”¨æ–¹æ³•ï¼‰
                chatOption.setConversationId(conversationId);
                String runtimeUuid = wfState.getUuid();

                // è®¾ç½® toolContextï¼Œä¼ é€’ç§Ÿæˆ·ç¼–ç å’Œç”¨æˆ·IDç»™ MCP å·¥å…·
                chatOption.setToolContext(Map.of(
                    "tenantCode", wfState.getTenantCode(),
                    "staffId", wfState.getUserId()
                ));

                log.info("LLM è°ƒç”¨å¼€å§‹ - conversationId: {}, runtimeUuid: {}, originalUserInput: {}, callSource: {}",
                        conversationId, runtimeUuid, originalUserInput, wfState.getCallSource());

                aiChatBaseService.chatWithWorkflowMemoryStream(chatOption, runtimeUuid, originalUserInput, wfState.getCallSource())
                        .chatResponse()
                        .doOnNext(chatResponse -> {
                            // åœ¨Reactoræµå›è°ƒä¸­è®¾ç½®ç§Ÿæˆ·ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢çº¿ç¨‹åˆ‡æ¢å¯¼è‡´ä¸Šä¸‹æ–‡ä¸¢å¤±
                            if (wfState.getTenantCode() != null) {
                                DataSourceHelper.use(wfState.getTenantCode());
                            }

                            String content = chatResponse.getResult().getOutput().getText();
                            if (StringUtils.isNotBlank(content)) {
                                log.debug("LLM chunk: length={}", content.length());
                                fullResponse.append(content);

                                if (wfState.getStreamHandler() != null) {
                                    wfState.getStreamHandler().sendNodeChunk(node.getUuid(), content);
                                }
                            }
                        })
                        .blockLast();

                log.info("LLM è°ƒç”¨å®Œæˆ - conversationId: {}, runtimeUuid: {}", conversationId, runtimeUuid);
            }

            String response = fullResponse.toString();
            // ç§»é™¤å‰å¯¼å’Œå°¾éšç©ºç™½å­—ç¬¦,é¿å…Markdownæ¸²æŸ“ä¸ºä»£ç å—
            if (StringUtils.isNotBlank(response)) {
                response = response.trim();
            }
            log.info("LLM streaming response completed, conversationId: {}, total length: {}",
                    conversationId, response.length());

            NodeIOData output = NodeIOData.createByText(DEFAULT_OUTPUT_PARAM_NAME, "", response);
            nodeState.getOutputs().add(output);
        } catch (Exception e) {
            log.error("invoke LLM (streaming) failed, conversationId: {}", conversationId, e);
            throw new RuntimeException("LLM æµå¼è°ƒç”¨å¤±è´¥: " + e.getMessage(), e);
        }
    }

    /**
     * æå–åŸå§‹ç”¨æˆ·è¾“å…¥
     *
     * ä»å·¥ä½œæµåˆå§‹è¾“å…¥ä¸­æå–var_user_inputå‚æ•°çš„å€¼ã€‚
     * è¿™æ˜¯ç”¨æˆ·åœ¨å¼€å§‹èŠ‚ç‚¹è¾“å…¥çš„åŸå§‹å†…å®¹ï¼Œç”¨äºä¿å­˜åˆ°å¯¹è¯å†å²è®°å½•ä¸­ã€‚
     *
     * @param wfState å·¥ä½œæµçŠ¶æ€å¯¹è±¡
     * @return åŸå§‹ç”¨æˆ·è¾“å…¥å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›null
     */
    private static String extractOriginalUserInput(WfState wfState) {
        if (wfState == null || wfState.getInput() == null) {
            log.warn("âŒ extractOriginalUserInput: wfStateæˆ–inputä¸ºnull");
            return null;
        }

        log.info("ğŸ” extractOriginalUserInput: å¼€å§‹æå–ç”¨æˆ·è¾“å…¥ï¼Œinputæ•°é‡: {}", wfState.getInput().size());

        // ä»å·¥ä½œæµåˆå§‹è¾“å…¥ä¸­æŸ¥æ‰¾var_user_inputå‚æ•°
        for (NodeIOData input : wfState.getInput()) {
            log.info("ğŸ” extractOriginalUserInput: æ£€æŸ¥input - name: {}, content: {}",
                input.getName(), input.getContent());

            if ("var_user_input".equals(input.getName())) {
                String value = input.valueToString();
                log.info("âœ… extractOriginalUserInput: æ‰¾åˆ°var_user_inputï¼Œvalue: {}", value);
                return value;
            }
        }

        log.warn("âŒ extractOriginalUserInput: æœªæ‰¾åˆ°var_user_inputå‚æ•°");
        return null;
    }

    // æ³¨æ„ï¼šUSERå’ŒASSISTANTæ¶ˆæ¯çš„ä¿å­˜å·²ç”±MessageChatMemoryAdvisorè‡ªåŠ¨ç®¡ç†
    // é€šè¿‡ScmWorkflowMessageChatMemory.add()æ–¹æ³•å®Œæˆ
    // ç¬¦åˆSpring AIæ¡†æ¶çš„æœ€ä½³å®è·µ

}
